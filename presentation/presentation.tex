\documentclass[aspectratio=169]{beamer}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}

\usetheme{Madrid}
\usecolortheme{seagull}

% Enable frame breaks for long content
\setbeamertemplate{continue label}{}

\title{Coupon Usage Prediction Model}
\subtitle{Machine Learning for Customer Transaction Analysis}
\author{Name Goes Here}
\institute{University Goes Here}
\date{\today}

\begin{document}

\section{Introduction}

\frame{\titlepage}

\begin{frame}{Problem Statement}
\textbf{Research Question:} Do transaction patterns and discount behaviors predict coupon usage?

\textbf{Goal:} Improve coupon targeting and reduce marketing waste

\textbf{Target Variable:} 
\begin{itemize}
    \item \texttt{coupon\_used} (1 = used, 0 = not used) - derived from coupon usage records
\end{itemize}

\textbf{Problem Type:} Binary classification
\begin{itemize}
    \item Features: transaction amounts, customer behavior, time patterns
    \item Response: whether a customer will use a coupon in a transaction
\end{itemize}
\end{frame}

\section{Data Description}

\begin{frame}{Data Description}
\textbf{Data Sources:}
\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
Dataset & Records & Description \\
\midrule
Wallet Transactions & 490,942 & Customer purchases \\
Coupon Usage & 75,676 & Coupons actually used \\
Coupon Distribution & 211,712 & Coupons sent to customers \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Patterns to Explore:}
\begin{itemize}
    \item Relationship between transaction amount and coupon usage
    \item Impact of discount depth on redemption rates
    \item Time-based patterns (hour, day of week)
    \item Customer behavior history as predictor
\end{itemize}
\end{frame}

\begin{frame}{Variable Types}
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Numeric (8):}
\begin{itemize}
    \item \texttt{tran\_amt}, \texttt{discounts\_amt}, \texttt{point\_amt}
    \item \texttt{coupon\_used\_count}, \texttt{total\_coupon\_used\_amt}
    \item \texttt{coupon\_send\_count}, \texttt{total\_coupon\_send\_amt}
    \item \texttt{benefit\_ratio}, \texttt{discount\_ratio}, \texttt{savings\_pct}
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Categorical (5):}
\begin{itemize}
    \item \texttt{station\_code}
    \item \texttt{attributionorgcode}
    \item \texttt{transactionorgcode}
    \item \texttt{hour}, \texttt{day\_of\_week}
    \item \texttt{is\_weekend}, \texttt{is\_morning}
\end{itemize}
\end{column}
\end{columns}

\textbf{Target Variable:} \texttt{coupon\_used} (Binary: 1 = Used, 0 = Not Used)
\end{frame}

\section{Data Preprocessing}

\begin{frame}[allowframebreaks]{Data Preprocessing}
\textbf{1. Feature Selection (Removed IDs/Hashes):}
\begin{itemize}
    \item \texttt{membercode}, \texttt{order\_no}, \texttt{external\_order\_no}
    \item \texttt{coupon\_code}, \texttt{user\_id}
\end{itemize}

\textbf{2. Outlier Removal (IQR Method):}
\begin{itemize}
    \item Method: Removed values outside 5th-95th percentile (IQR-based)
    \item Applied to: \texttt{tran\_amt}, \texttt{discounts\_amt}, \texttt{point\_amt}
    \item Why: Extreme values distort model training and visualization
    \item Records reduced: 490,942 $\rightarrow$ 421,590 (~14\% removed)
\end{itemize}

\textbf{3. Colinearity Removal:}
\begin{itemize}
    \item \texttt{receivable\_amt} (99\% correlated with \texttt{tran\_amt})
    \item \texttt{net\_amount}, \texttt{total\_benefit} (redundant)
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Features}
\textbf{Remaining Features (15):}
\begin{itemize}
    \item Transaction: \texttt{tran\_amt}, \texttt{discounts\_amt}, \texttt{point\_amt}
    \item Customer/Store: \texttt{station\_code}, \texttt{attributionorgcode}, \texttt{transactionorgcode}
    \item Time-based: \texttt{hour}, \texttt{day\_of\_week}, \texttt{is\_weekend}, \texttt{is\_morning}, etc.
    \item Aggregated: \texttt{coupon\_used\_count}, \texttt{total\_coupon\_used\_amt}
    \item \texttt{coupon\_send\_count}, \texttt{total\_coupon\_send\_amt}
\end{itemize}

\textbf{Engineered Features (5):}
\begin{itemize}
    \item \texttt{benefit\_ratio}, \texttt{discount\_ratio}
    \item \texttt{savings\_pct}, \texttt{point\_to\_discount\_ratio}
    \item \texttt{tran\_to\_receivable\_ratio}
\end{itemize}
\end{frame}

\begin{frame}{Feature Selection}
\textbf{Removed Irrelevant Features (IDs and Hashes):}
\begin{itemize}
    \item \texttt{membercode} - Customer ID (hash)
    \item \texttt{order\_no} - Transaction ID (hash, no date pattern)
    \item \texttt{external\_order\_no} - External ID (hash)
    \item \texttt{coupon\_code} - Coupon ID (hash)
    \item \texttt{user\_id} - User ID (hash)
\end{itemize}
\textbf{Impact:} Removing \texttt{membercode} improved accuracy from 64\% to 66\%
\end{frame}

\section{Function Definition and Implementation}

\begin{frame}{Key Libraries}
\begin{itemize}
    \item \textbf{pandas}: Data manipulation and analysis
    \item \textbf{numpy}: Numerical computations
    \item \textbf{sklearn}: Machine learning models and evaluation
    \item \textbf{tensorflow}: Neural network implementation
    \item \textbf{ matplotlib}, \textbf{seaborn}: Data visualization
\end{itemize}
\end{frame}

\begin{frame}{Sample Data: Raw Input}
\begin{table}
\centering
\begin{tabular}{lrrr}
\toprule
membercode & tran\_amt & discounts\_amt & coupon\_used \\
\midrule
M001 & 150.00 & 20.00 & 1 \\
M002 & 85.50 & 10.00 & 0 \\
M003 & 220.00 & 35.00 & 1 \\
M004 & 60.00 & 5.00 & 0 \\
M005 & 180.00 & 25.00 & 1 \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Sample Data: Engineered Features}
\begin{table}
\centering
\begin{tabular}{lrrrr}
\toprule
tran\_amt & discounts\_amt & benefit\_ratio & discount\_ratio & savings\_pct \\
\midrule
150.00 & 20.00 & 0.133 & 0.133 & 13.3\% \\
85.50 & 10.00 & 0.117 & 0.117 & 11.7\% \\
220.00 & 35.00 & 0.159 & 0.159 & 15.9\% \\
60.00 & 5.00 & 0.083 & 0.083 & 8.3\% \\
180.00 & 25.00 & 0.139 & 0.139 & 13.9\% \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Key Functions}
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Data Processing:}
\begin{itemize}
    \item \texttt{pd.read\_csv()}: Load datasets
    \item \texttt{df.merge()}: Join datasets
    \item \texttt{df.drop()}: Remove columns
    \item \texttt{df.quantile()}: Outlier removal
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Modeling:}
\begin{itemize}
    \item \texttt{train\_test\_split()}: Split data
    \item \texttt{GradientBoostingClassifier()}: Build model
    \item \texttt{cross\_val\_score()}: Validate
    \item \texttt{feature\_importances\_}: Get importance
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\section{Models}

\begin{frame}[allowframebreaks]{Model Architecture}
\begin{itemize}
    \item \textbf{Algorithms}:
    \begin{itemize}
        \item Random Forest Classifier
        \item Support Vector Machine (SVM) with RBF kernel
        \item Gradient Boosting Classifier
        \item Logistic Regression (AUC-based selection)
        \item Neural Network (3 hidden layers: 256-128-64)
    \end{itemize}
    \item \textbf{Random Forest Parameters}:
    \begin{itemize}
        \item 100 estimators
        \item Max depth: 20
    \end{itemize}
    \item \textbf{Gradient Boosting Parameters}:
    \begin{itemize}
        \item 100 iterations
        \item Max depth: 10
        \item Learning rate: 0.1
    \end{itemize}
    \item \textbf{Logistic Regression}:
    \begin{itemize}
        \item Balanced class weights
        \item AUC-based model selection
    \end{itemize}
    \item \textbf{Validation}: 3-Fold Cross Validation
    \item \textbf{Sample Size}: 20,000 records
\end{itemize}
\end{frame}

\begin{frame}{Neural Network: Class Weights}
\textbf{Impact of Class Weights on Neural Network:}

\bigskip
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Naive (Unbalanced) Weights: \{0:1, 1:2\}}
\begin{itemize}
    \item Accuracy: 62.92\%
    \item Recall: \textbf{92.06\%}
    \item Precision: 49.59\%
    \item Result: Predicts almost everything as positive
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Balanced Weights: \{0:1, 1:1\}}
\begin{itemize}
    \item Accuracy: \textbf{71.85\%}
    \item Recall: 58.11\%
    \item Precision: \textbf{62.29\%}
    \item Result: Better balance between precision and recall
\end{itemize}
\end{column}
\end{columns}

\bigskip
Key insight: Naive weights over-predict positives, achieving high recall but low precision. Balanced weights improve overall accuracy and create a more useful model.
\end{frame}

\begin{frame}{Model Performance (20,000 sample)}
\begin{table}
\centering
\begin{tabular}{lrrrrr}
\toprule
Metric & Random Forest & SVM & Gradient Boost & Logistic Reg & Neural Net \\
\midrule
Accuracy & \textbf{74.20\%} & 69.83\% & 73.78\% & 62.15\% & 71.85\% \\
AUC & \textbf{0.826} & 0.749 & 0.818 & 0.687 & 0.787 \\
Precision & 64.66\% & 61.10\% & 64.27\% & 48.64\% & 62.29\% \\
Recall & 64.75\% & 47.84\% & 63.52\% & 65.09\% & 58.11\% \\
F1 Score & \textbf{64.71\%} & 53.67\% & 63.89\% & 55.68\% & 60.13\% \\
\bottomrule
\end{tabular}
\end{table}
Random Forest achieves best accuracy and AUC. Neural Network improved with balanced class weights.
\end{frame}

\begin{frame}{Sample Size Comparison}
\begin{table}
\centering
\begin{tabular}{lcccc}
\toprule
Model & \shortstack{20,000\\Accuracy} & \shortstack{421,590\\Accuracy} & Improvement \\
\midrule
Random Forest & 74.22\% & \textbf{75.60\%} & +1.38\% \\
Gradient Boosting & 73.62\% & \textbf{74.13\%} & +0.51\% \\
\bottomrule
\end{tabular}
\end{table}
Larger sample size improves model accuracy, with Random Forest showing greater benefit from more data.
\end{frame}

\begin{frame}[allowframebreaks]{Confusion Matrices}

\textbf{Random Forest (Accuracy: 74.22\%):}
\[
\begin{bmatrix}
\sim 2100 & \sim 440 \\
\sim 580 & \sim 880
\end{bmatrix}
\]
\begin{center}
\begin{tabular}{lcc}
\toprule
  & Predicted: No & Predicted: Yes \\
\midrule
Actual: No & ~2100 (TN) & ~440 (FP) \\
Actual: Yes & ~580 (FN) & ~880 (TP) \\
\bottomrule
\end{tabular}
\end{center}

\pagebreak

\textbf{Gradient Boosting (Accuracy: 73.62\%):}
\[
\begin{bmatrix}
\sim 2080 & \sim 460 \\
\sim 590 & \sim 870
\end{bmatrix}
\]
\begin{center}
\begin{tabular}{lcc}
\toprule
  & Predicted: No & Predicted: Yes \\
\midrule
Actual: No & ~2080 (TN) & ~460 (FP) \\
Actual: Yes & ~590 (FN) & ~870 (TP) \\
\bottomrule
\end{tabular}
\end{center}

\pagebreak

\textbf{Neural Network (Accuracy: 71.85\%, Recall: 58.11\%):}
\[
\begin{bmatrix}
2025 & 514 \\
612 & 849
\end{bmatrix}
\]
\begin{center}
\begin{tabular}{lcc}
\toprule
  & Predicted: No & Predicted: Yes \\
\midrule
Actual: No & 2025 (TN) & 514 (FP) \\
Actual: Yes & 612 (FN) & 849 (TP) \\
\bottomrule
\end{tabular}
\end{center}
Neural Network improved with balanced class weights.
\end{frame}

\section{Feature Analysis}

\begin{frame}{Feature Importance}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{../images/feature_importance.png}
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
Top Features:
\begin{enumerate}
    \item \texttt{station\_code} (23.5\%)
    \item \texttt{hour} (9.45\%)
    \item \texttt{receivable\_amt} (8.36\%)
    \item \texttt{tran\_to\_receivable\_ratio} (8.29\%)
    \item \texttt{tran\_amt} (8.15\%)
    \item \texttt{day\_of\_week} (7.44\%)
    \item \texttt{attributionorgcode} (5.45\%)
    \item \texttt{total\_coupon\_send\_amt} (5.17\%)
    \item \texttt{total\_coupon\_used\_amt} (4.81\%)
    \item \texttt{coupon\_used\_count} (4.08\%)
\end{enumerate}
\vspace{0.3cm}
\end{column}
\end{columns}
\end{frame}
\begin{frame}
\textbf{How Feature Importance was Derived:}
\begin{itemize}
    \item Using Random Forest classifier's built-in feature importance
    \item Measures mean decrease in impurity (Gini importance)
    \item Averaged across all 100 decision trees
    \item Higher value = more predictive power
\end{itemize}
\end{frame}

\section{Visualization}

\begin{frame}{Correlation Matrix Heatmap}
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{../images/correlation_matrix.png}
\end{figure}
\end{frame}

\begin{frame}{Dimensionality Reduction}
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Density Comparison:}
\begin{itemize}
    \item Kernel density overlay of transaction amount vs hour
    \item Shows where each class concentrates
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{t-SNE Visualization:}
\begin{itemize}
    \item Non-linear embedding
    \item Preserves local structure
\end{itemize}
\end{column}
\end{columns}
\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{../images/dimensionality_reduction.png}
\end{figure}
\end{frame}

\begin{frame}{PCA Visualization}
\begin{figure}
\centering
\includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{../images/pca_visualization.png}
\end{figure}
\end{frame}

\section{Conclusions}

\begin{frame}[allowframebreaks]{Conclusions}
\begin{itemize}
    \item Random Forest achieves \textbf{74.20\% accuracy} with \textbf{0.826 AUC} - best model
    \item Gradient Boosting: 73.78\% accuracy, 0.818 AUC
    \item Neural Network: 71.85\% accuracy with balanced class weights (3 layers: 256-128-64)
    \item Feature selection: reduced from 23 to 15 features without accuracy loss
    \item Removed zero-importance features: \texttt{point\_amt}, \texttt{point\_to\_discount\_ratio}
    \item Top features: station\_code (23.5\%), attributionorgcode (9.4\%)
    \item 3-Fold Cross Validation confirms model stability
    \item Future work: XGBoost/LightGBM, more feature engineering
\end{itemize}
\end{frame}

\end{document}
